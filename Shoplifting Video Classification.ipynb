{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12674718,"sourceType":"datasetVersion","datasetId":8009813}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Shoplifting Detection from CCTV Videos\nThis notebook tackles the problem of detecting shoplifting activities from video footage. We will load a dataset of videos, preprocess them, and explore different deep learning models for classification.\n\nGoal: Classify a video as either containing 'Shoplifting' or 'No Shoplifting'.\n\n# 1. Setup and Imports\nFirst, let's import all the necessary libraries. We'll need OpenCV for video processing, NumPy for numerical operations, TensorFlow/Keras for building our models, and matplotlib for visualization.","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.layers import TimeDistributed, LSTM, GRU, ConvLSTM2D, Input, GlobalAveragePooling2D\nfrom tensorflow.keras.applications import MobileNetV2, EfficientNetV2B0\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.metrics import Precision, Recall, AUC\n\nprint(\"Libraries Imported!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T00:38:27.541968Z","iopub.execute_input":"2025-08-16T00:38:27.542272Z","iopub.status.idle":"2025-08-16T00:38:27.549020Z","shell.execute_reply.started":"2025-08-16T00:38:27.542249Z","shell.execute_reply":"2025-08-16T00:38:27.548238Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Data Loading and Preprocessing\nOur dataset is in two folders We need to:\n- Read each video file.\n- Extract a fixed number of frames from each video. Processing entire videos is computationally expensive, so we'll sample frames.\n- Resize frames to a consistent dimension.\n- Assign a label: 1 for Shoplifting, 0 for No Shoplifting.","metadata":{}},{"cell_type":"markdown","source":"- 70% of frames will be taken from the middle 50% of the video.\n\n- 30% of frames will be taken from the beginning and end sections.\n\nThis ensures our model sees the context (start and end) but focuses its attention on the most critical part of the video where the action happens.","metadata":{}},{"cell_type":"code","source":"# --- Configuration ---\nIMG_SIZE = 128  # Resize frames to 128x128\nMAX_FRAMES = 20 # Use a fixed number of frames from each video\nDATASET_PATH = \"/kaggle/input/shoplifting-videos-dataset/Shop DataSet\"\n\n# --- Define folder paths ---\nSHOPLIFTING_PATH = os.path.join(DATASET_PATH, \"shop lifters\")\nNO_SHOPLIFTING_PATH = os.path.join(DATASET_PATH, \"non shop lifters\")\n\n# --- UPDATED Data loading function with Biased Sampling ---\ndef load_videos(folder_path, label):\n    \"\"\"\n    Loads videos, extracts frames with a bias towards the middle, and assigns a label.\n    \"\"\"\n    video_files = os.listdir(folder_path)\n    sequences = []\n    labels = []\n    \n    print(f\"Processing folder: {folder_path}\")\n    for video_file in video_files:\n        video_path = os.path.join(folder_path, video_file)\n        cap = cv2.VideoCapture(video_path)\n        \n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        \n        # Skip videos that are too short to sample from\n        if total_frames < MAX_FRAMES:\n            cap.release()\n            continue\n\n        frames = []\n        \n        # --- Biased Frame Sampling Logic ---\n        # Define how many frames to take from the middle vs. edges\n        n_frames_middle = int(MAX_FRAMES * 0.7)  # e.g., 14 frames\n        n_frames_edges = MAX_FRAMES - n_frames_middle # e.g., 6 frames\n\n        # Define the video sections\n        middle_start_frame = int(total_frames * 0.25)\n        middle_end_frame = int(total_frames * 0.75)\n\n        # Generate indices for frames to extract\n        middle_indices = np.linspace(middle_start_frame, middle_end_frame, n_frames_middle, dtype=int)\n        edge_indices_start = np.linspace(0, middle_start_frame - 1, n_frames_edges // 2, dtype=int)\n        edge_indices_end = np.linspace(middle_end_frame + 1, total_frames - 1, n_frames_edges - (n_frames_edges // 2), dtype=int)\n        \n        # Combine indices, ensure they are unique and sorted\n        combined_indices = np.concatenate([edge_indices_start, middle_indices, edge_indices_end])\n        unique_indices = np.unique(combined_indices)\n        unique_indices.sort()\n\n        # Extract the frames at the calculated indices\n        for frame_idx in unique_indices:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n            ret, frame = cap.read()\n            if ret:\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n                frames.append(frame)\n        \n        cap.release()\n\n        # --- Padding / Truncating to ensure fixed length ---\n        # Ensure every sequence has exactly MAX_FRAMES\n        if len(frames) > MAX_FRAMES:\n            frames = frames[:MAX_FRAMES]\n        elif len(frames) < MAX_FRAMES and len(frames) > 0:\n            # Pad with the last frame if we have too few\n            padding = [frames[-1]] * (MAX_FRAMES - len(frames))\n            frames.extend(padding)\n\n        if len(frames) == MAX_FRAMES:\n            sequences.append(np.array(frames))\n            labels.append(label)\n            \n    return sequences, labels\n\n# --- Load the data ---\nprint(\"Loading 'Shoplifting' videos...\")\nshoplifting_sequences, shoplifting_labels = load_videos(SHOPLIFTING_PATH, 1)\nprint(f\"Loaded {len(shoplifting_sequences)} shoplifting videos.\")\n\nprint(\"\\nLoading 'No Shoplifting' videos...\")\nno_shoplifting_sequences, no_shoplifting_labels = load_videos(NO_SHOPLIFTING_PATH, 0)\nprint(f\"Loaded {len(no_shoplifting_sequences)} 'no shoplifting' videos.\")\n\n# --- Combine and prepare the final dataset ---\nX = np.array(shoplifting_sequences + no_shoplifting_sequences)\ny = np.array(shoplifting_labels + no_shoplifting_labels)\n\n# Normalize pixel values to be between 0 and 1\nX = X / 255.0\n\nprint(f\"\\nFinal dataset shape (Videos, Frames, Height, Width, Channels): {X.shape}\")\nprint(f\"Final labels shape: {y.shape}\")\n\n# --- Split into training and testing sets ---\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(f\"\\nTraining data shape: {X_train.shape}\")\nprint(f\"Testing data shape: {X_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T00:26:48.248140Z","iopub.execute_input":"2025-08-16T00:26:48.248554Z","iopub.status.idle":"2025-08-16T00:36:39.629259Z","shell.execute_reply.started":"2025-08-16T00:26:48.248537Z","shell.execute_reply":"2025-08-16T00:36:39.628544Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Visualization","metadata":{}},{"cell_type":"code","source":"def plot_sample_frames(X, y, class_name):\n    \"\"\"Plots sample frames for a given class.\"\"\"\n    plt.figure(figsize=(15, 6))\n    class_indices = np.where(y == (1 if class_name == \"Shoplifting\" else 0))[0]\n    \n    for i, idx in enumerate(class_indices[:5]): # Show first 5 videos of the class\n        video = X[idx]\n        # Show the 10th frame of the video\n        frame = video[10] \n        plt.subplot(2, 5, i + 1)\n        plt.imshow(frame)\n        plt.title(f\"{class_name} Video {i+1}\\nFrame 10\")\n        plt.axis('off')\n\n    plt.suptitle(f\"Sample Frames: {class_name}\", fontsize=16)\n    plt.show()\n\n# Visualize some training samples\nplot_sample_frames(X_train, y_train, \"Shoplifting\")\n\nplot_sample_frames(X_train, y_train, \"No Shoplifting\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T00:36:39.630147Z","iopub.execute_input":"2025-08-16T00:36:39.630821Z","iopub.status.idle":"2025-08-16T00:36:40.510927Z","shell.execute_reply.started":"2025-08-16T00:36:39.630787Z","shell.execute_reply":"2025-08-16T00:36:40.510315Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Model Suggestions\n## Model 1: 3DCNN Architecture","metadata":{}},{"cell_type":"code","source":"\n\ninput_shape = (MAX_FRAMES,IMG_SIZE, IMG_SIZE, 3)\n\nconv3d_model = Sequential([\n    Conv3D(32, (3, 3, 3), activation='relu', input_shape=input_shape),\n    MaxPooling3D(pool_size=(1, 2, 2)),\n    BatchNormalization(),\n\n    Conv3D(64, (3, 3, 3), activation='relu'),\n    MaxPooling3D(pool_size=(2, 2, 2)),\n    BatchNormalization(),\n\n    Conv3D(128, (3, 3, 3), activation='relu'),\n    MaxPooling3D(pool_size=(2, 2, 2)),\n    BatchNormalization(),\n\n    Flatten(),\n    Dense(256, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')\n])\n\nconv3d_model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall(), AUC()])\nconv3d_model.summary()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-15T11:27:36.777Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training The 3D Conv Model","metadata":{}},{"cell_type":"code","source":"history_3d = conv3d_model.fit(X_train, y_train,\n          validation_data=(X_test, y_test),\n          epochs=10,\n          batch_size=8,\n          callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)])\n","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-08-15T11:09:14.364414Z","shell.execute_reply.started":"2025-08-15T11:07:05.249527Z","shell.execute_reply":"2025-08-15T11:09:14.363702Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing results of 3D Conv Model","metadata":{}},{"cell_type":"code","source":"\ndef plot_training_history(history):\n    metrics = ['accuracy', 'precision', 'recall', 'auc', 'loss']\n    for metric in metrics:\n        train_metric = history.history.get(metric)\n        val_metric = history.history.get(f'val_{metric}')\n\n        if train_metric is not None:\n            plt.figure()\n            plt.plot(train_metric, label=f'Train {metric.capitalize()}')\n            if val_metric is not None:\n                plt.plot(val_metric, label=f'Val {metric.capitalize()}')\n            plt.xlabel('Epoch')\n            plt.ylabel(metric.capitalize())\n            plt.title(f'{metric.capitalize()} over Epochs')\n            plt.legend()\n            plt.grid(True)\n            plt.show()\nplot_training_history(history_3d)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:09:14.365758Z","iopub.execute_input":"2025-08-15T11:09:14.366575Z","iopub.status.idle":"2025-08-15T11:09:15.214448Z","shell.execute_reply.started":"2025-08-15T11:09:14.366548Z","shell.execute_reply":"2025-08-15T11:09:15.213798Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model 2 : Pretrained CNN + LSTM","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, TimeDistributed, LSTM, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n\n\ndef build_cnn_plus_lstm_model():\n    input_shape = (MAX_FRAMES,IMG_SIZE, IMG_SIZE, 3)\n    input_layer= Input(shape=input_shape)\n\n    cnn_base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n    for layer in cnn_base.layers:\n        layer.trainable=False\n        \n    cnn_out = TimeDistributed(cnn_base)(input_layer)\n    cnn_out = TimeDistributed(GlobalAveragePooling2D())(cnn_out)\n    cnn_out = TimeDistributed(BatchNormalization())(cnn_out)\n\n    lstm_out = LSTM(128, return_sequences=False)(cnn_out)\n    x = Dropout(0.5)(lstm_out)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    output = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=input_layer, outputs=output)\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:09:15.215209Z","iopub.execute_input":"2025-08-15T11:09:15.215426Z","iopub.status.idle":"2025-08-15T11:09:15.222398Z","shell.execute_reply.started":"2025-08-15T11:09:15.215409Z","shell.execute_reply":"2025-08-15T11:09:15.221647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lstm_model = build_cnn_plus_lstm_model()\nlstm_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:09:15.223217Z","iopub.execute_input":"2025-08-15T11:09:15.223497Z","iopub.status.idle":"2025-08-15T11:09:16.456773Z","shell.execute_reply.started":"2025-08-15T11:09:15.223469Z","shell.execute_reply":"2025-08-15T11:09:16.455896Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training the CNN + LSTM Model","metadata":{}},{"cell_type":"code","source":"lstm_model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall(), AUC()])\n\nhistory_lstm = lstm_model.fit(X_train, y_train,\n          validation_data=(X_test, y_test),\n          epochs=10,\n          batch_size=4,\n          callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:09:16.457694Z","iopub.execute_input":"2025-08-15T11:09:16.457990Z","iopub.status.idle":"2025-08-15T11:15:46.013572Z","shell.execute_reply.started":"2025-08-15T11:09:16.457963Z","shell.execute_reply":"2025-08-15T11:15:46.012711Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing The Result Of the Model","metadata":{}},{"cell_type":"code","source":"plot_training_history(history_lstm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:15:46.017065Z","iopub.execute_input":"2025-08-15T11:15:46.017369Z","iopub.status.idle":"2025-08-15T11:15:46.350317Z","shell.execute_reply.started":"2025-08-15T11:15:46.017347Z","shell.execute_reply":"2025-08-15T11:15:46.349627Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model 3: Transformer","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import (\n    Input, TimeDistributed, Dense, Dropout, GlobalAveragePooling2D,\n    LayerNormalization, MultiHeadAttention, Add\n)\n\ndef transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n    attention = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)\n    attention = Dropout(dropout)(attention)\n    x = Add()([inputs, attention])\n    x = LayerNormalization(epsilon=1e-6)(x)\n\n    ff = Dense(ff_dim, activation='relu')(x)\n    ff = Dropout(dropout)(ff)\n    ff = Dense(inputs.shape[-1])(ff)\n    x = Add()([x, ff])\n    x = LayerNormalization(epsilon=1e-6)(x)\n    return x\n\ndef build_cnn_plus_transformer():\n    input_shape = (MAX_FRAMES, IMG_SIZE, IMG_SIZE, 3)\n    input_layer = Input(shape=input_shape)\n\n    cnn_base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n    cnn_base.trainable = False\n\n    x = TimeDistributed(cnn_base)(input_layer)\n    x = TimeDistributed(GlobalAveragePooling2D())(x)\n\n    x = transformer_encoder(x, head_size=64, num_heads=4, ff_dim=128, dropout=0.1)\n\n    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    output = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=input_layer, outputs=output)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:15:46.351150Z","iopub.execute_input":"2025-08-15T11:15:46.351439Z","iopub.status.idle":"2025-08-15T11:15:46.359774Z","shell.execute_reply.started":"2025-08-15T11:15:46.351411Z","shell.execute_reply":"2025-08-15T11:15:46.359091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transformer_model = build_cnn_plus_transformer()\ntransformer_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:15:46.360751Z","iopub.execute_input":"2025-08-15T11:15:46.361048Z","iopub.status.idle":"2025-08-15T11:15:47.192218Z","shell.execute_reply.started":"2025-08-15T11:15:46.361027Z","shell.execute_reply":"2025-08-15T11:15:47.191383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntransformer_model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall(), AUC()])\n\nhistory_transformer = transformer_model.fit(X_train, y_train,\n          validation_data=(X_test, y_test),\n          epochs=10,\n          batch_size=4,\n          callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:15:47.193138Z","iopub.execute_input":"2025-08-15T11:15:47.193429Z","iopub.status.idle":"2025-08-15T11:20:49.510909Z","shell.execute_reply.started":"2025-08-15T11:15:47.193398Z","shell.execute_reply":"2025-08-15T11:20:49.510041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_training_history(history_transformer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:20:49.512433Z","iopub.execute_input":"2025-08-15T11:20:49.512678Z","iopub.status.idle":"2025-08-15T11:20:49.849758Z","shell.execute_reply.started":"2025-08-15T11:20:49.512658Z","shell.execute_reply":"2025-08-15T11:20:49.849061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras import backend as K\nimport gc\n\n# After finishing with your model\nK.clear_session()\ndel transformer_model\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:20:49.850498Z","iopub.execute_input":"2025-08-15T11:20:49.850784Z","iopub.status.idle":"2025-08-15T11:20:55.537888Z","shell.execute_reply.started":"2025-08-15T11:20:49.850764Z","shell.execute_reply":"2025-08-15T11:20:55.537175Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### More pretrained models","metadata":{}},{"cell_type":"code","source":"def build_cnn_plus_lstm_model(input_shape = (MAX_FRAMES, IMG_SIZE, IMG_SIZE, 3)):\n    # Define the input layer\n    input_layer = Input(shape=input_shape, name=\"input_layer\")\n\n    # 1. Spatial Feature Extraction (CNN)\n    # Load MobileNetV2, pre-trained on ImageNet, without its final classifier\n    cnn_base = MobileNetV2(weights='imagenet', include_top=False, \n                           input_shape=input_shape[1:]) # Shape of a single frame\n    \n    # Freeze the layers of the pre-trained base\n    cnn_base.trainable = False\n    \n    # Apply the CNN to each frame of the video sequence\n    # The output is a sequence of feature maps\n    cnn_out = TimeDistributed(cnn_base, name=\"mobilenet_feature_extractor\")(input_layer)\n    \n    # Flatten the spatial features for each frame into a single vector\n    cnn_out = TimeDistributed(GlobalAveragePooling2D(), name=\"feature_vector_pooling\")(cnn_out)\n\n    # 2. Temporal Feature Processing (RNN)\n    # The LSTM processes the sequence of feature vectors\n    lstm_out = LSTM(128, return_sequences=False, name=\"lstm_temporal_processor\")(cnn_out)\n    \n    # 3. Classifier Head\n    x = Dropout(0.5)(lstm_out)\n    x = Dense(64, activation='relu')(x)\n    output = Dense(1, activation='sigmoid', name=\"output_layer\")(x)\n\n    # Create the final model\n    model = Model(inputs=input_layer, outputs=output, name=\"MobileNetV2_plus_LSTM\")\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:20:55.539548Z","iopub.execute_input":"2025-08-15T11:20:55.539775Z","iopub.status.idle":"2025-08-15T11:20:55.575771Z","shell.execute_reply.started":"2025-08-15T11:20:55.539758Z","shell.execute_reply":"2025-08-15T11:20:55.575140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cnn_lstm_model = build_cnn_plus_lstm_model()\ncnn_lstm_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:20:55.576579Z","iopub.execute_input":"2025-08-15T11:20:55.576783Z","iopub.status.idle":"2025-08-15T11:20:56.652785Z","shell.execute_reply.started":"2025-08-15T11:20:55.576767Z","shell.execute_reply":"2025-08-15T11:20:56.652010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ncnn_lstm_model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall(), AUC()])\n\nhistory_cnn_lstm = cnn_lstm_model.fit(X_train, y_train,\n          validation_data=(X_test, y_test),\n          epochs=10,\n          batch_size=4,\n          callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:20:56.653686Z","iopub.execute_input":"2025-08-15T11:20:56.653969Z","iopub.status.idle":"2025-08-15T11:27:18.387657Z","shell.execute_reply.started":"2025-08-15T11:20:56.653944Z","shell.execute_reply":"2025-08-15T11:27:18.386955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_training_history(history_cnn_lstm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:27:18.390197Z","iopub.execute_input":"2025-08-15T11:27:18.390447Z","iopub.status.idle":"2025-08-15T11:27:19.236811Z","shell.execute_reply.started":"2025-08-15T11:27:18.390426Z","shell.execute_reply":"2025-08-15T11:27:19.235974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras import backend as K\nimport gc\n\n# After finishing with your model\nK.clear_session()\ndel cnn_lstm_model\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:27:19.237684Z","iopub.execute_input":"2025-08-15T11:27:19.237978Z","iopub.status.idle":"2025-08-15T11:27:27.333585Z","shell.execute_reply.started":"2025-08-15T11:27:19.237951Z","shell.execute_reply":"2025-08-15T11:27:27.332822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_efficientnet_plus_gru_model(input_shape = (MAX_FRAMES, IMG_SIZE, IMG_SIZE, 3)):\n    # Define the input layer\n    input_layer = Input(shape=input_shape, name=\"input_layer\")\n\n    # 1. Spatial Feature Extraction (CNN)\n    # Load EfficientNetV2B0, pre-trained on ImageNet\n    cnn_base = EfficientNetV2B0(weights='imagenet', include_top=False, \n                                input_shape=input_shape[1:])\n    \n    # Freeze the layers of the pre-trained base\n    cnn_base.trainable = False\n    \n    # Apply the CNN to each frame of the video sequence\n    cnn_out = TimeDistributed(cnn_base, name=\"efficientnet_feature_extractor\")(input_layer)\n    \n    # Flatten the spatial features for each frame\n    cnn_out = TimeDistributed(GlobalAveragePooling2D(), name=\"feature_vector_pooling\")(cnn_out)\n\n    # 2. Temporal Feature Processing (RNN)\n    # The GRU processes the sequence of feature vectors\n    gru_out = GRU(128, return_sequences=False, name=\"gru_temporal_processor\")(cnn_out)\n    \n    # 3. Classifier Head\n    x = Dropout(0.5)(gru_out)\n    x = Dense(64, activation='relu')(x)\n    output = Dense(1, activation='sigmoid', name=\"output_layer\")(x)\n\n    # Create the final model\n    model = Model(inputs=input_layer, outputs=output, name=\"EfficientNetV2_plus_GRU\")\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T00:37:08.275301Z","iopub.execute_input":"2025-08-16T00:37:08.275553Z","iopub.status.idle":"2025-08-16T00:37:08.281354Z","shell.execute_reply.started":"2025-08-16T00:37:08.275534Z","shell.execute_reply":"2025-08-16T00:37:08.280491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"efficientnet_gru_model = build_efficientnet_plus_gru_model()\nefficientnet_gru_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T00:37:11.547491Z","iopub.execute_input":"2025-08-16T00:37:11.548187Z","iopub.status.idle":"2025-08-16T00:37:17.051320Z","shell.execute_reply.started":"2025-08-16T00:37:11.548163Z","shell.execute_reply":"2025-08-16T00:37:17.050768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nefficientnet_gru_model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall(), AUC()])\n\nhistory_efficientnet_gru = efficientnet_gru_model.fit(X_train, y_train,\n          validation_data=(X_test, y_test),\n          epochs=10,\n          batch_size=4,\n          callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T00:38:33.661973Z","iopub.execute_input":"2025-08-16T00:38:33.662254Z","iopub.status.idle":"2025-08-16T00:50:25.711550Z","shell.execute_reply.started":"2025-08-16T00:38:33.662233Z","shell.execute_reply":"2025-08-16T00:50:25.710910Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_training_history(history_efficientnet_gru)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Saving Models","metadata":{}},{"cell_type":"code","source":"conv3d_model.save('conv3d.h5')\nlstm_model.save('lstm_model.h5')\ntransformer_model.save('transformer_model.h5')\ncnn_lstm_model.save('cnn_lstm_model.h5')\nefficientnet_gru_model.save('efficientnet_gru_model.h5')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-15T11:27:36.778Z"}},"outputs":[],"execution_count":null}]}